{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 檢測GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('GPU:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 亂數模擬資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Loaded sample [0]\n",
      "  Features : [10, 37, 48, 45, 12, 44, 12]\n",
      "  Label    : 1\n",
      "------------------------------------------------------------\n",
      "Loaded sample [1]\n",
      "  Features : [19, 17, 6, 12, 39, 11, 21]\n",
      "  Label    : 0\n",
      "------------------------------------------------------------\n",
      "Loaded data shape : (10000, 7)\n",
      "Loaded labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取 CSV 檔案並印出前幾筆資料\n",
    "df = pd.read_csv('simulated_data.csv')\n",
    "\n",
    "loaded_data = df.iloc[:, :-1].values  # 所有特徵欄位\n",
    "loaded_labels = df.iloc[:, -1].values  # 標籤欄位\n",
    "\n",
    "for i in range(2):\n",
    "    print('-' * 60)\n",
    "    print(f'Loaded sample [{i}]')\n",
    "    print(f'  Features : {loaded_data[i].tolist()}')\n",
    "    print(f'  Label    : {loaded_labels[i]}')\n",
    "print('-' * 60)\n",
    "print(f'Loaded data shape : {loaded_data.shape}')  # 使用shape一定要是array或tensor\n",
    "print(f'Loaded labels shape: {loaded_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正規化資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Data Range\n",
      "  Min    : 1\n",
      "  Max    : 49\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 找到資料集的上下限\n",
    "data_min = loaded_data.min()\n",
    "data_max = loaded_data.max()\n",
    "print('-' * 60)\n",
    "print(f'Data Range')\n",
    "print(f'  Min    : {data_min}')\n",
    "print(f'  Max    : {data_max}')\n",
    "print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Original sample [0]\n",
      "  Data   : ['10.00000', '37.00000', '48.00000', '45.00000', '12.00000', '44.00000', '12.00000']\n",
      "------------------------------------------------------------\n",
      "Normalized sample [0]\n",
      "  Data   : ['0.18750', '0.75000', '0.97917', '0.91667', '0.22917', '0.89583', '0.22917']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def normalize_sequences(sequences):\n",
    "    min_val = sequences.min()\n",
    "    max_val = sequences.max()\n",
    "    return (sequences - min_val) / (max_val - min_val)\n",
    "\n",
    "# 正規化處理\n",
    "normalized_data = normalize_sequences(loaded_data)\n",
    "\n",
    "# 顯示前後\n",
    "sample_index = 0\n",
    "print('-' * 60)\n",
    "print(f'Original sample [{sample_index}]')\n",
    "print(f'  Data   : {[f\"{v:.5f}\" for v in loaded_data[sample_index].tolist()]}')\n",
    "print('-' * 60)\n",
    "print(f'Normalized sample [{sample_index}]')\n",
    "print(f'  Data   : {[f\"{v:.5f}\" for v in normalized_data[sample_index].tolist()]}')\n",
    "print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Train data count      : 8000\n",
      "Validation data count : 2000\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 使用 sklearn 分割資料\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    normalized_data, loaded_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 顯示資料切割\n",
    "print('-' * 60)\n",
    "print('Train data count      :', len(train_data))\n",
    "print('Validation data count :', len(val_data))\n",
    "print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Sample from train_dataset\n",
      "Input   - Shape : 7\n",
      "          Data  : [0.04166667 0.95833333 0.83333333 0.10416667 0.52083333 0.45833333\n",
      " 0.60416667]\n",
      "Target  - Label : 1\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 自訂資料集類別，回傳可變長度序列\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        # 初始化資料與標籤\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # 回傳資料集的大小\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 回傳 dict 格式包含序列與標籤\n",
    "        return {\n",
    "            'input': self.data[idx],\n",
    "            'target': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# 建立資料集實例\n",
    "train_dataset = MyDataset(train_data, train_labels)\n",
    "val_dataset = MyDataset(val_data, val_labels)\n",
    "\n",
    "# 顯示資料集中第一筆樣本資訊\n",
    "sample = train_dataset[0]\n",
    "print('\\n' + '-' * 60)\n",
    "print('Sample from train_dataset')\n",
    "print(f\"{'Input':<8}- Shape : {len(sample['input'])}\\n{'':<10}Data  : {sample['input']}\")\n",
    "print(f\"{'Target':<8}- Label : {sample['target']}\")\n",
    "print('-' * 60 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Sample after applying my_collate_fn\n",
      "\n",
      "Input     - Shape : torch.Size([2, 7])\n",
      "            Data  :\n",
      "tensor([[0.0417, 0.9583, 0.8333, 0.1042, 0.5208, 0.4583, 0.6042],\n",
      "        [0.5417, 0.3958, 0.8125, 0.2083, 0.1250, 0.9792, 0.7500]])\n",
      "\n",
      "Target    - Label : tensor([1, 1])\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_36692\\2900401265.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  'input':  torch.tensor(inputs, dtype=torch.float32),\n"
     ]
    }
   ],
   "source": [
    "def my_collate_fn(batch):\n",
    "    inputs  = [item['input'] for item in batch]\n",
    "    targets = [item['target'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'input':  torch.tensor(inputs, dtype=torch.float32),\n",
    "        'target': torch.tensor(targets, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# 使用 sample 套用自訂的 collate_fn，這裡取第 0 筆與第 1 筆資料\n",
    "batch           = [train_dataset[0], train_dataset[1]]\n",
    "collated_sample = my_collate_fn(batch)\n",
    "\n",
    "print('\\n' + '-' * 60)\n",
    "print('Sample after applying my_collate_fn\\n')\n",
    "print(f\"{'Input':<10}- Shape : {collated_sample['input'].shape}\")\n",
    "print(f\"{'':<10}  Data  :\\n{collated_sample['input']}\\n\")\n",
    "print(f\"{'Target':<10}- Label : {collated_sample['target']}\")\n",
    "print('-' * 60 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Sample after applying my_collate_fn with DataLoader\n",
      "\n",
      "Input     - Shape : torch.Size([5, 7])\n",
      "            Data  :\n",
      "tensor([[0.1250, 0.1667, 0.0625, 0.4583, 0.1250, 0.7292, 0.7708],\n",
      "        [0.2708, 0.5417, 0.8542, 0.1458, 0.8958, 0.7917, 0.4792],\n",
      "        [0.4167, 0.8750, 0.0000, 0.2292, 0.1250, 0.2500, 0.2292],\n",
      "        [0.0417, 0.3750, 0.8542, 0.3333, 0.0000, 0.3750, 0.6250],\n",
      "        [0.5417, 0.6875, 0.3958, 0.0625, 0.5625, 0.8333, 0.2708]])\n",
      "\n",
      "Target    - Label : tensor([0, 1, 0, 0, 0])\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset    = train_dataset,\n",
    "    batch_size = 5,\n",
    "    shuffle    = True,\n",
    "    collate_fn = my_collate_fn\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset    = val_dataset,\n",
    "    batch_size = 5,\n",
    "    shuffle    = False,\n",
    "    collate_fn = my_collate_fn\n",
    ")\n",
    "\n",
    "# 取出一個 batch 的資料進行檢查（只取第一個）\n",
    "collated_sample = next(iter(train_loader))\n",
    "print('\\n' + '-' * 60)\n",
    "print('Sample after applying my_collate_fn with DataLoader\\n')\n",
    "print(f\"{'Input':<10}- Shape : {collated_sample['input'].shape}\")\n",
    "print(f\"{'':<10}  Data  :\\n{collated_sample['input']}\\n\")\n",
    "print(f\"{'Target':<10}- Label : {collated_sample['target']}\")\n",
    "print('-' * 60 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 建立 MLP 模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden)  # 全連接層，將輸入從 input_size 維轉換為 hidden 維\n",
    "        self.fc2 = nn.Linear(hidden, output_size)  # 全連接層，輸出為 output 類別\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        x = self.fc1(kwargs['input'])  # 第一次線性變換\n",
    "        x = F.relu(x)  # ReLU 非線性轉換\n",
    "        x = self.fc2(x)  # 第二次線性變換，輸出 logits\n",
    "        loss = self.loss_fn(x, kwargs['target'])  # 計算 loss\n",
    "        \n",
    "        return loss, x\n",
    "\n",
    "model = MLP(input_size=train_data.shape[1], hidden=4, output_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.6316, Train Acc: 0.6465, Val Loss: 0.5349, Val Acc: 0.7870\n",
      "Epoch [2/10], Train Loss: 0.4126, Train Acc: 0.9217, Val Loss: 0.3371, Val Acc: 0.9140\n",
      "Epoch [3/10], Train Loss: 0.2701, Train Acc: 0.9666, Val Loss: 0.2358, Val Acc: 0.9890\n",
      "Epoch [4/10], Train Loss: 0.2014, Train Acc: 0.9774, Val Loss: 0.1873, Val Acc: 0.9750\n",
      "Epoch [5/10], Train Loss: 0.1628, Train Acc: 0.9814, Val Loss: 0.1577, Val Acc: 0.9710\n",
      "Epoch [6/10], Train Loss: 0.1388, Train Acc: 0.9829, Val Loss: 0.1362, Val Acc: 0.9820\n",
      "Epoch [7/10], Train Loss: 0.1219, Train Acc: 0.9879, Val Loss: 0.1201, Val Acc: 0.9905\n",
      "Epoch [8/10], Train Loss: 0.1094, Train Acc: 0.9870, Val Loss: 0.1093, Val Acc: 0.9910\n",
      "Epoch [9/10], Train Loss: 0.1001, Train Acc: 0.9880, Val Loss: 0.1006, Val Acc: 0.9915\n",
      "Epoch [10/10], Train Loss: 0.0921, Train Acc: 0.9885, Val Loss: 0.0982, Val Acc: 0.9740\n"
     ]
    }
   ],
   "source": [
    "# 訓練函數\n",
    "def train_one_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss, outputs = model(**batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        correct += (predicted == batch['target']).sum().item()\n",
    "        total += batch['target'].size(0)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 驗證函數\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            loss, outputs = model(**batch)\n",
    "            total_loss += loss.item()\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            correct += (predicted == batch['target']).sum().item()\n",
    "            total += batch['target'].size(0)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 執行訓練流程\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_acc = validate(model, valid_loader)\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'.format(\n",
    "        epoch + 1, num_epochs, train_loss, train_acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次梯度： tensor([[1.4492, 2.8984]])\n",
      "第二次未清除梯度： tensor([[2.8984, 5.7968]])\n",
      "第三次清除後的梯度： tensor([[1.4492, 2.8984]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def run_backward_and_print(title, clear_grad):\n",
    "    if clear_grad:\n",
    "        optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    print(title, model.weight.grad)\n",
    "\n",
    "# 建立一個簡單模型\n",
    "model = nn.Linear(2, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 假資料\n",
    "x = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "y = torch.tensor([[1.0]])\n",
    "\n",
    "run_backward_and_print(\"第一次梯度：\", clear_grad=True)\n",
    "run_backward_and_print(\"第二次未清除梯度：\", clear_grad=False)\n",
    "run_backward_and_print(\"第三次清除後的梯度：\", clear_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
