{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# XOR 問題的輸入資料\n",
    "x_data = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# XOR 的標籤\n",
    "y_data = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # 初始化第一層的權重與偏差（輸入層 -> 隱藏層）\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.random.randn(hidden_size)\n",
    "        # 初始化第二層的權重與偏差（隱藏層 -> 輸出層）\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.random.randn(output_size)\n",
    "\n",
    "        # 設定學習率\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) # 避免指數爆炸\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Softmax 函數，轉換為機率分佈（公式2.4）\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # 前向傳播：輸入層 -> 隱藏層（公式2.1）\n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "\n",
    "        # 使用 ReLU 激活函數（公式2.2）\n",
    "        a1 = np.maximum(0, z1)\n",
    "\n",
    "        # 隱藏層 -> 輸出層（公式2.3）\n",
    "        z2 = np.dot(a1, self.w2) + self.b2\n",
    "\n",
    "        # 輸出使用 softmax 激活函數轉為機率（公式2.4）\n",
    "        y_hat = self.softmax(z2)\n",
    "        \n",
    "        # one-hot 編碼\n",
    "        y_onehot = np.zeros_like(y_hat)\n",
    "        y_onehot[np.arange(len(y)), y] = 1\n",
    "\n",
    "        # 計算交叉熵損失（公式2.5）\n",
    "        loss = -np.sum(y_onehot * np.log(y_hat + 1e-9)) / len(y)  # 加入 1e-9 避免 log(0)\n",
    "        return loss, y_hat, z1, a1, y_onehot\n",
    "\n",
    "    def backward(self, x, y_onehot, y_hat, z1, a1):\n",
    "        # 輸出層梯度（公式2.6）\n",
    "        # ∂L/∂Z^(2) = (Y_hat - Y) / N\n",
    "        dz2 = (y_hat - y_onehot) / len(x)\n",
    "\n",
    "        # ∂L/∂W^(2) = A^(1).T · ∂L/∂Z^(2)\n",
    "        dw2 = np.dot(a1.T, dz2)\n",
    "\n",
    "        # ∂L/∂b^(2) = sum over batch of ∂L/∂Z^(2)\n",
    "        db2 = np.sum(dz2, axis=0)\n",
    "\n",
    "        # 隱藏層梯度（公式2.9）\n",
    "        # ∂L/∂Z^(1) = ((Y_hat - Y) · W^(2).T) ⊙ ReLU'(Z^(1))\n",
    "        da1 = np.dot(dz2, self.w2.T)\n",
    "        relu_grad = (z1 > 0).astype(float)\n",
    "        dz1 = da1 * relu_grad\n",
    "\n",
    "        # ∂L/∂W^(1) = X.T · ∂L/∂Z^(1)\n",
    "        dw1 = np.dot(x.T, dz1)\n",
    "\n",
    "        # ∂L/∂b^(1) = sum over batch of ∂L/∂Z^(1)\n",
    "        db1 = np.sum(dz1, axis=0)\n",
    "\n",
    "        # 梯度更新\n",
    "        self.w2 -= self.lr * dw2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.w1 -= self.lr * dw1\n",
    "        self.b1 -= self.lr * db1\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 前向傳播：輸入層 -> 隱藏層\n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "\n",
    "        # ReLU 激活\n",
    "        a1 = np.maximum(0, z1)\n",
    "\n",
    "        # 隱藏層 -> 輸出層\n",
    "        z2 = np.dot(a1, self.w2) + self.b2\n",
    "\n",
    "        # softmax 轉為機率\n",
    "        y_hat = self.softmax(z2)\n",
    "\n",
    "        return y_hat, np.argmax(y_hat, axis=1)\n",
    "    \n",
    "# 初始化兩個 MLP 模型用於比較SGD與GD\n",
    "model_sgd = MLP(input_size=x_data.shape[1], hidden_size=4, output_size=2, learning_rate=0.1)\n",
    "model_gd = MLP(input_size=x_data.shape[1], hidden_size=4, output_size=2, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, SGD Loss: 0.011743, GD Loss: 0.007088\n",
      "Epoch 2000, SGD Loss: 0.017702, GD Loss: 0.002655\n",
      "Epoch 3000, SGD Loss: 0.011249, GD Loss: 0.001577\n",
      "Epoch 4000, SGD Loss: 0.001909, GD Loss: 0.001099\n",
      "Epoch 5000, SGD Loss: 0.000714, GD Loss: 0.000834\n",
      "Epoch 6000, SGD Loss: 0.001088, GD Loss: 0.000667\n",
      "Epoch 7000, SGD Loss: 0.004398, GD Loss: 0.000553\n",
      "Epoch 8000, SGD Loss: 0.000422, GD Loss: 0.000471\n",
      "Epoch 9000, SGD Loss: 0.000673, GD Loss: 0.000409\n",
      "Epoch 10000, SGD Loss: 0.000293, GD Loss: 0.000361\n"
     ]
    }
   ],
   "source": [
    "def train(model_sgd, model_gd, x_data, y_data, epochs=10000):\n",
    "    sgd_losses = []\n",
    "    gd_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # SGD：隨機抽取一筆資料\n",
    "        idx = np.random.randint(0, len(x_data))\n",
    "        x = x_data[idx].reshape(1, -1)\n",
    "        y = np.array([y_data[idx]])\n",
    "    \n",
    "        loss, y_hat, z1, a1, y_onehot = model_sgd.forward(x, y)\n",
    "        model_sgd.backward(x, y_onehot, y_hat, z1, a1)\n",
    "\n",
    "        sgd_losses.append(loss)\n",
    "\n",
    "        # GD：使用整批資料更新一次參數\n",
    "        loss_gd, y_hat_gd, z1_gd, a1_gd, y_onehot_gd = model_gd.forward(x_data, y_data)\n",
    "        model_gd.backward(x_data, y_onehot_gd, y_hat_gd, z1_gd, a1_gd)\n",
    "\n",
    "        gd_losses.append(loss_gd)\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, SGD Loss: {loss:.6f}, GD Loss: {loss_gd:.6f}\")\n",
    "\n",
    "    return sgd_losses, gd_losses\n",
    "\n",
    "# 訓練模型並獲得 SGD 與 GD 的損失\n",
    "sgd_losses, gd_losses = train(model_sgd, model_gd, x_data, y_data, epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Results (SGD):\n",
      "Input: [0 0], Output: [9.99381620e-01 6.18379867e-04], Predicted Class: 0\n",
      "Input: [0 1], Output: [0.00294014 0.99705986], Predicted Class: 1\n",
      "Input: [1 0], Output: [2.90835325e-04 9.99709165e-01], Predicted Class: 1\n",
      "Input: [1 1], Output: [9.99517405e-01 4.82595152e-04], Predicted Class: 0\n",
      "\n",
      "Prediction Results (GD):\n",
      "Input: [0 0], Output: [9.99530671e-01 4.69328505e-04], Predicted Class: 0\n",
      "Input: [0 1], Output: [2.66116355e-04 9.99733884e-01], Predicted Class: 1\n",
      "Input: [1 0], Output: [3.8168974e-04 9.9961831e-01], Predicted Class: 1\n",
      "Input: [1 1], Output: [9.99675409e-01 3.24590633e-04], Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# 預測並輸出結果（SGD 模型）\n",
    "print(\"\\nPrediction Results (SGD):\")\n",
    "for i in range(len(x_data)):\n",
    "    x = x_data[i].reshape(1, -1)\n",
    "    y_hat, pred = model_sgd.predict(x)\n",
    "    print(f\"Input: {x_data[i]}, Output: {y_hat[0]}, Predicted Class: {pred[0]}\")\n",
    "\n",
    "# 預測並輸出結果（GD 模型）\n",
    "print(\"\\nPrediction Results (GD):\")\n",
    "for i in range(len(x_data)):\n",
    "    x = x_data[i].reshape(1, -1)\n",
    "    y_hat, pred = model_gd.predict(x)\n",
    "    print(f\"Input: {x_data[i]}, Output: {y_hat[0]}, Predicted Class: {pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
