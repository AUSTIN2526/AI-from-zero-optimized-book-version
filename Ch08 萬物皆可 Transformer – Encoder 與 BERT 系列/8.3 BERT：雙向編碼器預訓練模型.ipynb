{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformer_layers import Encoder\n",
    " \n",
    "class BertInputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, vocab_size=30522, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertInputEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)                 # 詞嵌入：將 token id 轉換為向量\n",
    "        self.position = nn.Embedding(max_position_embeddings, d_model)     # 位置嵌入：表示 token 在序列中的相對位置\n",
    "        self.token_type = nn.Embedding(type_vocab_size, d_model)           # 句段嵌入：用來區分句子 A/B 的 segment embeddings\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-12)                       # LayerNorm：正規化處理穩定訓練\n",
    "        self.dropout = nn.Dropout(dropout)                                 # dropout：防止過擬合\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        # 加總詞嵌入、位置嵌入與句段嵌入\n",
    "        x = self.embedding(input_ids) + self.position(position_ids) + self.token_type(token_type_ids)\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertPooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 只取[CLS] token (序列的第一個token)\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, d_model=768, nhead=12, d_ff=3072, num_layers=12, dropout=0.1, \n",
    "                 vocab_size=30522, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # 核心組件\n",
    "        self.embedding = BertInputEmbedding(d_model, dropout, vocab_size, max_position_embeddings, type_vocab_size)\n",
    "        self.encoder = Encoder(d_model, nhead, d_ff, num_layers, dropout)\n",
    "        self.pooler = BertPooler(d_model)\n",
    "        \n",
    "        # 初始化權重\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 使用截斷正態分佈初始化線性層權重\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, return_dict=False):\n",
    "        # 輸入嵌入\n",
    "        embedding_output = self.embedding(input_ids, token_type_ids)\n",
    "        \n",
    "        # 處理attention mask\n",
    "        if attention_mask is not None:\n",
    "            src_key_padding_mask = attention_mask == 0\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        \n",
    "        # 編碼器處理\n",
    "        sequence_output = self.encoder(embedding_output, mask=None, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Pooler處理\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        \n",
    "        if return_dict:\n",
    "            return {\n",
    "                'last_hidden_state': sequence_output,\n",
    "                'pooler_output': pooled_output\n",
    "            }\n",
    "        else:\n",
    "            return sequence_output, pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預訓練頭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNSPHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(BertNSPHead, self).__init__()\n",
    "        self.seq_relationship = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score\n",
    "    \n",
    "class BertMLMHead(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(BertMLMHead, self).__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.norm = nn.RMSNorm(d_model, eps=1e-12)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "        self.predictions = BertMLMHead(d_model, vocab_size)\n",
    "        self.seq_relationship = BertNSPHead(d_model)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score\n",
    "    \n",
    "\n",
    "class BertForPreTraining(nn.Module):\n",
    "    def __init__(self, d_model=768, nhead=12, d_ff=3072, num_layers=12, dropout=0.1,\n",
    "                 vocab_size=30522, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertForPreTraining, self).__init__()\n",
    "        self.bert = BertModel(d_model, nhead, d_ff, num_layers, dropout, \n",
    "                             vocab_size, max_position_embeddings, type_vocab_size)\n",
    "        self.cls = BertPreTrainingHeads(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, next_sentence_label=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids, return_dict=True)\n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        \n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "        \n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            # MLM Loss\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.bert.vocab_size), labels.view(-1))\n",
    "            \n",
    "            # NSP Loss\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            \n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'prediction_logits': prediction_scores,\n",
    "            'seq_relationship_logits': seq_relationship_score,\n",
    "            'hidden_states': sequence_output,\n",
    "            'pooler_output': pooled_output\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分類模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, d_model=768, nhead=12, d_ff=3072, num_layers=12, dropout=0.1,\n",
    "                 vocab_size=30522, max_position_embeddings=512, type_vocab_size=2, num_labels=2):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(d_model, nhead, d_ff, num_layers, dropout,\n",
    "                             vocab_size, max_position_embeddings, type_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids, return_dict=True)\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'hidden_states': outputs['last_hidden_state'],\n",
    "            'pooler_output': pooled_output\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 測試基本BERT模型 ===\n",
      "Sequence output shape: torch.Size([1, 6, 768])\n",
      "Pooled output shape: torch.Size([1, 768])\n",
      "\n",
      "=== 測試預訓練模型 ===\n",
      "Prediction logits shape: torch.Size([1, 6, 30522])\n",
      "NSP logits shape: torch.Size([1, 2])\n",
      "\n",
      "=== 測試分類模型 ===\n",
      "Classification logits shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 測試用範例\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 測試基本BERT模型\n",
    "print(\"=== 測試基本BERT模型 ===\")\n",
    "sentence = \"this is a test\"\n",
    "tokens = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n",
    "token_type_ids = tokens['token_type_ids']\n",
    "\n",
    "model = BertModel()\n",
    "sequence_output, pooled_output = model(**tokens)\n",
    "print(f\"Sequence output shape: {sequence_output.shape}\")  # [batch_size, seq_len, d_model]\n",
    "print(f\"Pooled output shape: {pooled_output.shape}\")      # [batch_size, d_model]\n",
    "\n",
    "# 測試預訓練模型\n",
    "print(\"\\n=== 測試預訓練模型 ===\")\n",
    "pretraining_model = BertForPreTraining()\n",
    "outputs = pretraining_model(**tokens)\n",
    "print(f\"Prediction logits shape: {outputs['prediction_logits'].shape}\")      # [batch_size, seq_len, vocab_size]\n",
    "print(f\"NSP logits shape: {outputs['seq_relationship_logits'].shape}\")       # [batch_size, 2]\n",
    "\n",
    "# 測試分類模型\n",
    "print(\"\\n=== 測試分類模型 ===\")\n",
    "classification_model = BertForSequenceClassification(num_labels=3)\n",
    "outputs = classification_model(**tokens)\n",
    "print(f\"Classification logits shape: {outputs['logits'].shape}\")             # [batch_size, num_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
