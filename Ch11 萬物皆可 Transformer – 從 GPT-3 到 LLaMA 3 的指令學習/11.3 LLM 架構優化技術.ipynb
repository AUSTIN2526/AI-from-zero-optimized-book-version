{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7ee42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=8192, base=10000):\n",
    "        super(RoPE, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        \n",
    "        # 預計算頻率值 (frequencies)\n",
    "        # inv_freq = 1 / (base^(2i/d_model)) for i in [0, 1, ..., d_model//2-1]\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # 預計算 cos 和 sin 表，避免重複計算\n",
    "        self._precompute_freqs(max_seq_len)\n",
    "    \n",
    "    def _precompute_freqs(self, seq_len):\n",
    "        # 位置索引 [0, 1, 2, ..., seq_len-1]\n",
    "        t = torch.arange(seq_len, dtype=torch.float32)\n",
    "        \n",
    "        # 計算每個位置和每個頻率的乘積\n",
    "        # freqs shape: (seq_len, d_model//2)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        \n",
    "        # 將頻率重複兩次以匹配 d_model 維度\n",
    "        # emb shape: (seq_len, d_model)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        \n",
    "        # 預計算 cos 和 sin\n",
    "        self.register_buffer('cos_cached', emb.cos(), persistent=False)\n",
    "        self.register_buffer('sin_cached', emb.sin(), persistent=False)\n",
    "    \n",
    "    def _rotate_half(self, x):\n",
    "        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    def forward(self, x, seq_len=None, start_pos=0):\n",
    "        if seq_len is None:\n",
    "            seq_len = x.shape[-2]\n",
    "        \n",
    "        # 如果序列長度超過預計算的長度，重新計算\n",
    "        if start_pos + seq_len > self.cos_cached.shape[0]:\n",
    "            self._precompute_freqs(start_pos + seq_len)\n",
    "        \n",
    "        # 取得對應位置的 cos 和 sin 值\n",
    "        cos = self.cos_cached[start_pos:start_pos + seq_len]\n",
    "        sin = self.sin_cached[start_pos:start_pos + seq_len]\n",
    "        \n",
    "        # 擴展維度以匹配輸入張量的形狀\n",
    "        # cos, sin: (seq_len, d_model) -> (..., seq_len, d_model)\n",
    "        cos = cos.unsqueeze(0).expand_as(x)\n",
    "        sin = sin.unsqueeze(0).expand_as(x)\n",
    "        \n",
    "        # 應用旋轉變換: x_rotated = x * cos + rotate_half(x) * sin\n",
    "        return x * cos + self._rotate_half(x) * sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0768a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class GatedFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.1, activation = 'gelu'):\n",
    "        super(GatedFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff * 2)  # 輸出兩倍維度，作為 GLU 輸入\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)      # 第二次線性變換，降回原始維度\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'silu' or activation == 'swish':\n",
    "            self.activation = F.silu\n",
    "        elif callable(activation):\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            raise ValueError('Unsupported activation function: {}'.format(activation))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 將輸出切成兩半：一部分作為輸出，一部分作為 gate\n",
    "        x_proj = self.linear1(x)\n",
    "        x1, x2 = x_proj.chunk(2, dim=-1)\n",
    "        x_gated = x1 * self.activation(x2)  # 套用 gating 機制\n",
    "        return self.linear2(self.dropout(x_gated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263fcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_kv_heads, dropout=0.1, max_seq_len=8192):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        assert d_model % nhead == 0, \"d_model 必須可以整除 nhead\"\n",
    "        assert nhead % num_kv_heads == 0, \"nhead 必須可以整除 num_kv_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead  # query heads 數量\n",
    "        self.num_kv_heads = num_kv_heads  # key-value heads 數量\n",
    "        self.d_k = d_model // nhead  # 每個 head 的維度大小\n",
    "        self.num_queries_per_kv = nhead // num_kv_heads  # 每個 KV head 對應的 Q head 數量\n",
    "        \n",
    "        # Query 仍然使用全部的 heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        # Key 和 Value 只使用較少的 heads\n",
    "        self.w_k = nn.Linear(d_model, num_kv_heads * self.d_k)\n",
    "        self.w_v = nn.Linear(d_model, num_kv_heads * self.d_k)\n",
    "        self.w_o = nn.Linear(d_model, d_model)  # 最終輸出映射層\n",
    "        \n",
    "        # 為每個 head 建立 RoPE\n",
    "        self.rope = RoPE(self.d_k, max_seq_len)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)  # 縮放因子，用於穩定 softmax\n",
    "    \n",
    "    def apply_rope_to_heads(self, x, seq_len, start_pos=0):\n",
    "        batch_size, num_heads, seq_len_actual, d_k = x.shape\n",
    "        \n",
    "        # 使用 contiguous() 確保記憶體連續，然後重塑為 (batch_size * num_heads, seq_len, d_k)\n",
    "        x_reshaped = x.contiguous().view(batch_size * num_heads, seq_len_actual, d_k)\n",
    "        \n",
    "        # 應用 RoPE\n",
    "        x_rope = self.rope(x_reshaped, seq_len, start_pos)\n",
    "        \n",
    "        # 重塑回 (batch_size, num_heads, seq_len, d_k)\n",
    "        return x_rope.view(batch_size, num_heads, seq_len_actual, d_k)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None, key_padding_mask=None, start_pos=0):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        kv_seq_len = key.size(1)\n",
    "        \n",
    "        # Query: (batch_size, nhead, seq_len, d_k)\n",
    "        Q = self.w_q(query).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Key 和 Value: (batch_size, num_kv_heads, kv_seq_len, d_k)\n",
    "        K = self.w_k(key).view(batch_size, kv_seq_len, self.num_kv_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, kv_seq_len, self.num_kv_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 對 Q 和 K 應用 RoPE (V 不需要位置編碼)\n",
    "        Q = self.apply_rope_to_heads(Q, seq_len, start_pos)\n",
    "        K = self.apply_rope_to_heads(K, kv_seq_len, start_pos)\n",
    "        \n",
    "        # 將 K 和 V 重複擴展以匹配 Q 的 head 數量\n",
    "        # 使用 repeat_interleave 讓每個 KV head 對應多個 Q head\n",
    "        K = K.repeat_interleave(self.num_queries_per_kv, dim=1)  # (batch_size, nhead, kv_seq_len, d_k)\n",
    "        V = V.repeat_interleave(self.num_queries_per_kv, dim=1)  # (batch_size, nhead, kv_seq_len, d_k)\n",
    "        \n",
    "        # 計算注意力分數\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # 套用 attention mask（如 causal mask）\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        # 套用 padding mask，忽略 padding 位置\n",
    "        if key_padding_mask is not None:\n",
    "            scores = scores.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf')\n",
    "            )\n",
    "        \n",
    "        # 計算注意力權重\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 計算 weighted sum 後 reshape 回原始形狀\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # 最終輸出線性變換\n",
    "        output = self.w_o(context)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
