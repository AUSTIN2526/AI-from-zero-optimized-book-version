{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6322a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_causal_mask(seq_len, device=None):\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    if device is not None:\n",
    "        mask = mask.to(device)\n",
    "    return mask\n",
    "\n",
    "def create_padding_mask(seq_lengths, max_len):\n",
    "    batch_size = len(seq_lengths)\n",
    "    mask = torch.zeros(batch_size, max_len, dtype=torch.bool)\n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        if length < max_len:\n",
    "            mask[i, length:] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009483c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformer_layers import MultiHeadAttention, FeedForward\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, num_layers, dropout=0.1, norm=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'self_attn': MultiHeadAttention(d_model, nhead, dropout),\n",
    "                'cross_attn': MultiHeadAttention(d_model, nhead, dropout),\n",
    "                'feed_forward': FeedForward(d_model, d_ff, dropout),\n",
    "                'norm1': nn.LayerNorm(d_model),\n",
    "                'norm2': nn.LayerNorm(d_model),\n",
    "                'norm3': nn.LayerNorm(d_model),\n",
    "                'dropout': nn.Dropout(dropout)\n",
    "            }) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        output = tgt\n",
    "        for layer in self.layers:\n",
    "            # 自注意力機制（含遮罩）+ 殘差連接 + LayerNorm\n",
    "            tgt2 = layer['self_attn'](output, output, output, tgt_mask, tgt_key_padding_mask)\n",
    "            output = layer['norm1'](output + layer['dropout'](tgt2))\n",
    "            \n",
    "            # Cross attention（如果有 memory）\n",
    "            if memory is not None:\n",
    "                tgt2 = layer['cross_attn'](output, memory, memory, memory_mask, memory_key_padding_mask)\n",
    "                output = layer['norm2'](output + layer['dropout'](tgt2))\n",
    "            \n",
    "            # 前饋神經網路 + 殘差連接 + LayerNorm\n",
    "            tgt2 = layer['feed_forward'](output)\n",
    "            output = layer['norm3'](output + layer['dropout'](tgt2))\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 測試用模型建立與輸入資料\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "decoder = Decoder(d_model, nhead, d_ff, num_layers)\n",
    "tgt = torch.randn(batch_size, seq_len, d_model)\n",
    "memory = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 建立遮罩\n",
    "tgt_mask = create_causal_mask(seq_len).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "tgt_key_padding_mask = create_padding_mask([10, 8], seq_len)\n",
    "memory_key_padding_mask = create_padding_mask([10, 7], seq_len)\n",
    "\n",
    "output = decoder(\n",
    "    tgt, \n",
    "    memory, \n",
    "    tgt_mask=tgt_mask, \n",
    "    memory_mask=None, \n",
    "    tgt_key_padding_mask=tgt_key_padding_mask, \n",
    "    memory_key_padding_mask=memory_key_padding_mask\n",
    ")\n",
    "print(output.shape)  # 預期輸出: (batch_size, seq_len, d_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
